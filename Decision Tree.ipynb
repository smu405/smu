{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decisoin Tree\n",
    "\n",
    "\n",
    "* 의사결정에 영향을 미치는 속성을 값에 따라 분기하여, 최종 결정에 이르도록 함 (사다리타기와 유사).\n",
    "\n",
    "## ID3 algorithm\n",
    "\n",
    "* input: Training Data, Attribute List\n",
    "  * 학습데이터\n",
    "  * 속성목록\n",
    "* output: decision tree\n",
    "\n",
    "Generate_decision_tree(Training Data, Attribute List)\n",
    "* create a node N 시작 노드\n",
    "* if samples are all of the same class C then\n",
    "    return N as a leaf node labeled with class C; 모두 같은 클래스인 경우 분기하여 노드\n",
    "* if attribute-list is empty then\n",
    "    return N as a leaft node labeled with majority voting; 분기할 속성이 남아 있지 않으면 최빈으로 노드\n",
    "* from among attributes in the attribute-list,\n",
    "    select test attribute that leads to the highest information gain\n",
    "    label node N with the test-attribute; IG가 가장 높은 속성을 선택하여 노드\n",
    "* for each known value ai of test-attribute //partition the samples\n",
    "    * grow a branch from node N for the condition test-attribute = ai 분기\n",
    "    * let si be the set of samples in the samples for which test-attribute = ai 데이터분할\n",
    "    * if si is empty then\n",
    "        attach a leaf labelled with majority voting; 분할한 데이터가 공집합이면 최빈\n",
    "    * else\n",
    "        attach the node returned by Generate_decision_tree(si, Attribute List - test attribute); 재귀적으로 함수를 실행하고 그 결과 값을 받아서 노드\n",
    "}\n",
    "\n",
    "## 프로그래밍\n",
    "\n",
    "1. 데이터준비\n",
    "2. 엔트로피 계산\n",
    "3. Information Gain 계산\n",
    "4. decision tree 구조 만듦\n",
    "5. 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shannon entorpy\n",
    "\n",
    "* 정보이론 (Shannon, 1948)\n",
    "* Entropy H(S)는 데이터에 포함된 불확실성, 엔트로피가 클수록 불확실성이 높음.\n",
    "* log2를 취하면서 그 승수가 정보를 표현하는 bit수를 의미.\n",
    "즉 4의 log2 값은 2 ($2^2$), 4개의 정보 값을 표현하려면 2 bits 필요.\n",
    "\n",
    "참고\n",
    "* scipy.stats.entropy\n",
    "* Gini impurity\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H(S) &= - \\sum_{i=1}^{n}\\ p_i\\ log_2\\ p_i\\\\\n",
    "     &= - p_1 log_2\\ p_1 - p_2 log_2\\ p_2\\ \\ldots\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "위 식에서 $p_i$는 사건i가 발생할 확률을 의미.\n",
    "동전을 던지는 경우, 모두 앞면 또는 모두 뒷면은 entropy가 0 (즉 불확실성이 없는 purity)\n",
    "반대로 반반씩 섞여 있는 경우 entropy는 가장 크다.\n",
    "즉 엔트로피가 클수록 정보가 혼재되어 있다는 의미.\n",
    "가장 유용한 정보는 엔트로피를 가장 많이 감소시키는 것.\n",
    "\n",
    "* H( (0.5,0.5) ) = -0.5 x log_2 0.5 - 0.5 x log_2 0.5 = 1\n",
    "* H( (0.5,0.5) ) = -0.7 x log_2 0.7 - 0.3 x log_2 0.3 = 0.88\n",
    "* H( (0.5,0.5) ) = -1.0 x log_2 1.0 - 0.0 x log_2 0.0 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.5]\n",
      "[0.3602012209808308, 0.5210896782498619]\n",
      "math domain error\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print [-p*math.log(p,2) for p in [0.5,0.5]]\n",
    "print [-p*math.log(p,2) for p in [0.7,0.3]]\n",
    "try:\n",
    "    [-p*math.log(p,2) for p in [1.0,0.0]]\n",
    "except:\n",
    "    print \"math domain error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문자열의 예\n",
    "\n",
    "* 엔트로피는 부호화에 필요한 문자 당 최소 평균 이진값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 빈도 2 확률 0.153846153846 엔트로피 0.415452264329 엔트로피누적 0.415452264329\n",
      "c 빈도 1 확률 0.0769230769231 엔트로피 0.284649209088 엔트로피누적 0.700101473417\n",
      "b 빈도 1 확률 0.0769230769231 엔트로피 0.284649209088 엔트로피누적 0.984750682505\n",
      "e 빈도 1 확률 0.0769230769231 엔트로피 0.284649209088 엔트로피누적 1.26939989159\n",
      "d 빈도 4 확률 0.307692307692 엔트로피 0.523212220966 엔트로피누적 1.79261211256\n",
      "g 빈도 1 확률 0.0769230769231 엔트로피 0.284649209088 엔트로피누적 2.07726132165\n",
      "f 빈도 3 확률 0.230769230769 엔트로피 0.488187050174 엔트로피누적 2.56544837182\n",
      "entropy=[0.52877123795494485, 0.44217935649972373]\n",
      "funct=0.970950594455\n"
     ]
    }
   ],
   "source": [
    "# compute entropy with a string example\n",
    "import numpy as np\n",
    "import math\n",
    "str = 'aabcddddefffg'\n",
    "# 1) count frequency\n",
    "allChars=list(str)\n",
    "#uniqueChars=set(allChars)\n",
    "#dictionary to save frequencies of all characters\n",
    "tokenVector=dict()\n",
    "for token in allChars:\n",
    "    # increase if the key exists\n",
    "    if tokenVector.has_key(token):\n",
    "        tokenVector[token]+=1\n",
    "    else:\n",
    "        tokenVector[token]=1\n",
    "# 2) computes entropy\n",
    "entropy=0\n",
    "allFreq=float(len(allChars))\n",
    "for key in tokenVector.iterkeys():\n",
    "    freq=tokenVector[key]\n",
    "    prob=float(freq)/allFreq\n",
    "    ent=-prob * math.log(prob,2)\n",
    "    entropy=entropy+ent\n",
    "    print \"{0} 빈도 {1} 확률 {2} 엔트로피 {3} 엔트로피누적 {4}\".format(key,freq,prob,ent,entropy)\n",
    "\n",
    "# 묶어서 함수로\n",
    "# yes가 2/5 no가 3/5인 경우 (교재 p.42)\n",
    "S=np.array( (0.4, 0.6))\n",
    "entropy=[ -p*math.log(p,2) for p in S ]\n",
    "def getEntropy(data):\n",
    "    entropy=[-p*math.log(p,2) for p in data]\n",
    "    return sum(entropy)\n",
    "#0.97095059\n",
    "print \"entropy={0}\\nfunct={1}\".format(entropy,getEntropy(S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## information gain\n",
    "\n",
    "* 불확실성이 가장 낮은 속성으로 분기하기 위한 방법. 가장 높은 IG를 선택하여 분기함.\n",
    "* 속성선정값 (attribute selection measure) 또는 분기적합도 (measure of the goodness of split)\n",
    "\n",
    "$$\n",
    "IG(A,S) = H(S) - \\sum_{i=1}^m\\ p(i)\\ H(i)\n",
    "$$\n",
    "\n",
    "* 각 속성의 값으로, 그 발생확률을 계산 (위 식 $p(i)$)\n",
    "* 그 속성 값의 엔트로피를 계산 (H(i)\n",
    "* 위 1과 2를 가중하여 해당 속성의 엔트로피를 계산 (위 식의 우측 항)\n",
    "* 기본엔트로피 (H(s))에서 공제하여 Information Gain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제\n",
    "\n",
    "* 속성 4개 열, 속성 값은 문자열\n",
    "* 클래스 마지막 열\n",
    "* 테이블로 나타내면:\n",
    "\n",
    "| age | has_job | own_house | credit_rating | class |\n",
    "|-----|---------|-----------|---------------|-------|\n",
    "| young | false | false | fair | no |\n",
    "\n",
    "* numpy로 계산 (List로 처리하는 것과 비교)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['young' 'false' 'false' 'fair' 'No']\n",
      " ['young' 'false' 'false' 'good' 'No']\n",
      " ['young' 'true' 'false' 'good' 'Yes']\n",
      " ['young' 'true' 'true' 'fair' 'Yes']\n",
      " ['young' 'false' 'false' 'fair' 'No']\n",
      " ['middle' 'false' 'false' 'fair' 'No']\n",
      " ['middle' 'false' 'false' 'good' 'No']\n",
      " ['middle' 'true' 'true' 'good' 'Yes']\n",
      " ['middle' 'false' 'true' 'excellent' 'Yes']\n",
      " ['middle' 'false' 'true' 'excellent' 'Yes']\n",
      " ['old' 'false' 'true' 'excellent' 'Yes']\n",
      " ['old' 'false' 'true' 'good' 'Yes']\n",
      " ['old' 'true' 'false' 'good' 'Yes']\n",
      " ['old' 'true' 'false' 'excellent' 'Yes']\n",
      " ['old' 'false' 'false' 'fair' 'No']]\n"
     ]
    }
   ],
   "source": [
    "S=np.array([['young', 'false', 'false', 'fair', 'No'],\n",
    "       ['young', 'false', 'false', 'good', 'No'],\n",
    "       ['young', 'true', 'false', 'good', 'Yes'],\n",
    "       ['young', 'true', 'true', 'fair', 'Yes'],\n",
    "       ['young', 'false', 'false', 'fair', 'No'],\n",
    "       ['middle', 'false', 'false', 'fair', 'No'],\n",
    "       ['middle', 'false', 'false', 'good', 'No'],\n",
    "       ['middle', 'true', 'true', 'good', 'Yes'],\n",
    "       ['middle', 'false', 'true', 'excellent', 'Yes'],\n",
    "       ['middle', 'false', 'true', 'excellent', 'Yes'],\n",
    "       ['old', 'false', 'true', 'excellent', 'Yes'],\n",
    "       ['old', 'false', 'true', 'good', 'Yes'],\n",
    "       ['old', 'true', 'false', 'good', 'Yes'],\n",
    "       ['old', 'true', 'false', 'excellent', 'Yes'],\n",
    "       ['old', 'false', 'false', 'fair', 'No']], \n",
    "      dtype='|S9')\n",
    "print S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 엔트로피 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나이['middle' 'old' 'young'] 빈도[5 5 5]\n"
     ]
    }
   ],
   "source": [
    "#학습데이터 첫째 속성(나이)의 엔트로피계산을 위한 확률 구함.\n",
    "#키를 찾아, 빈도를 계산\n",
    "a=S[:,0]\n",
    "keys=np.unique(a)\n",
    "bins=keys.searchsorted(S[:,0])\n",
    "freq=np.bincount(bins)\n",
    "print \"나이{0} 빈도{1}\".format(keys,freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫째 속성의 키/빈도수 [['middle' 'old' 'young']\n",
      " ['5' '5' '5']]\n",
      "마지막 컬럼 클래스의 키/빈도수 [['No' 'Yes']\n",
      " ['6' '9']]\n",
      "(2, 2)\n",
      "확률 [ 0.4  0.6] 전체빈도 15\n",
      "엔트로피는 0.970950594455\n",
      "\n",
      "\t확률 [ 0.4  0.6] 전체빈도 15\n",
      "\t엔트로피는 0.970950594455\n",
      "함수를 이용한 엔트로피는 0.970950594455\n"
     ]
    }
   ],
   "source": [
    "#함수로 구현\n",
    "#in: array (n x 1) (속성 컬럼)\n",
    "#out: array(2 x n) (키의 빈도 row1=keys, row2:frequencies)\n",
    "def findKeyCounts(data):\n",
    "    #find keys in nominal data\n",
    "    keys=np.unique(data)\n",
    "    #find indices for data (if sorted)\n",
    "    bins=keys.searchsorted(data)\n",
    "    #count for each key index\n",
    "    #count returned as strings. conversion does not work?\n",
    "    #All items in a numpy array have to have the same dtype.\n",
    "    #possibly use a numpy recarray\n",
    "    #return np.vstack([keys,np.bincount(bins).astype(np.int)])\n",
    "    return np.vstack([keys,np.bincount(bins)])\n",
    "print \"첫째 속성의 키/빈도수 {0}\".format(findKeyCounts(S[:,0]))\n",
    "#pandas를 사용해서 구하려면, pd.value_counts(a[:,0])\n",
    "\n",
    "#전체 데이터에 대해 엔트로피를 계산하면\n",
    "#1) 확률 계산\n",
    "#Yes: 9/15 No: 6/15\n",
    "print \"마지막 컬럼 클래스의 키/빈도수 {0}\".format(findKeyCounts(S[:,-1]))\n",
    "\n",
    "#확률을 구하려면, 전체빈도수를 계산.\n",
    "kc=findKeyCounts(S[:,-1])\n",
    "print kc.shape\n",
    "#array는 문자열이 섞이면 수도 문자열로 casting.\n",
    "allFreq=kc[1,:].astype('int').sum()\n",
    "#vector연산이므로 for-loop가 필요없슴.\n",
    "prob=kc[1,:].astype('float')/allFreq\n",
    "print \"확률 {0} 전체빈도 {1}\".format(prob,allFreq)\n",
    "\n",
    "#2) 엔트로피를 계산\n",
    "print \"엔트로피는 {0}\".format(sum([-p*math.log(p,2) for p in prob]))\n",
    "\n",
    "#in: array (n x 1 속성컬럼)\n",
    "#out: 엔트로피값\n",
    "#교재 calcShannonEnt(dataSet)과 동일한 기능이지만 입력은 1개 컬럼.\n",
    "def getEntropy1(data):\n",
    "  kc=findKeyCounts(data)\n",
    "  allFreq=kc[1,:].astype('int').sum()\n",
    "  prob=kc[1,:].astype('float')/allFreq\n",
    "  print \"\\n\\t확률 {0} 전체빈도 {1}\".format(prob,allFreq)\n",
    "  entropy=sum([-p*math.log(p,2) for p in prob])\n",
    "  print \"\\t엔트로피는 {0}\".format(entropy)\n",
    "  return entropy\n",
    "\n",
    "print \"함수를 이용한 엔트로피는 {0}\".format(getEntropy1(S[:,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Information Gain 계산\n",
    "\n",
    "### 우선 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['young' 'false' 'false' 'fair' 'No']\n",
      " ['young' 'false' 'false' 'good' 'No']\n",
      " ['young' 'true' 'false' 'good' 'Yes']\n",
      " ['young' 'true' 'true' 'fair' 'Yes']\n",
      " ['young' 'false' 'false' 'fair' 'No']]\n",
      "[['No' 'Yes']\n",
      " ['3' '2']]\n",
      "[['No' 'Yes']\n",
      " ['2' '3']]\n",
      "[['No' 'Yes']\n",
      " ['1' '4']]\n",
      "Young 엔트로피 0.323650198152 = 0.333333333333 곱하기 0.970950594455\n",
      "Middle 엔트로피 0.323650198152 = 0.333333333333 곱하기 0.970950594455\n",
      "Old 엔트로피 0.240642698296 = 0.333333333333 곱하기 0.721928094887\n",
      "나이 엔트로피: 0.887943094599 = 0.323650198152 + 0.323650198152 + 0.240642698296\n",
      "전체 클래스 엔트로피: 0.970950594455\n",
      "나이 information gain: 0.0830074998558 = 0.970950594455 - 0.887943094599\n"
     ]
    }
   ],
   "source": [
    "#3) 어느 속성으로 분할할 지 비교\n",
    "#나이Age의 경우\n",
    "\n",
    "#H(age)=5/15 * H(age=young) + 5/15 * H(age=middle) + 5/15 * H(age=old)\n",
    "#=0.888\n",
    "\n",
    "#컬럼의 조건에 따른 행선택\n",
    "inData=S\n",
    "sub=inData[inData[:,0]=='young']\n",
    "print sub\n",
    "#array([['young', 'false', 'false', 'fair', 'No'],\n",
    "#       ['young', 'false', 'false', 'good', 'No'],\n",
    "#       ['young', 'true', 'false', 'good', 'Yes'],\n",
    "#       ['young', 'true', 'true', 'fair', 'Yes'],\n",
    "#       ['young', 'false', 'false', 'fair', 'No']])\n",
    "print findKeyCounts(sub[:,-1])\n",
    "#array([['No', 'Yes'],['3', '2']])\n",
    "freqYoung=len(sub)\n",
    "entYoung=sum([-p*math.log(p,2) for p in (3/5.,2/5.)])\n",
    "#0.9709505944546686\n",
    "\n",
    "sub=inData[inData[:,0]=='middle']\n",
    "#array([['middle', 'false', 'false', 'fair', 'No'],\n",
    "#       ['middle', 'false', 'false', 'good', 'No'],\n",
    "#       ['middle', 'true', 'true', 'good', 'Yes'],\n",
    "#       ['middle', 'false', 'true', 'excellent', 'Yes'],\n",
    "#       ['middle', 'false', 'true', 'excellent', 'Yes']]) \n",
    "print findKeyCounts(sub[:,-1])\n",
    "#array([['No', 'Yes'],['2', '3']]) \n",
    "freqMiddle=len(sub)\n",
    "entMiddle=sum([-p*math.log(p,2) for p in (2/5.,3/5.)])\n",
    "#0.9709505944546686\n",
    "\n",
    "sub=inData[inData[:,0]=='old']\n",
    "print findKeyCounts(sub[:,-1])\n",
    "#array([['No', 'Yes'],['1', '4']])\n",
    "freqOld=len(sub)\n",
    "entOld=sum([-p*math.log(p,2) for p in (1/5.,4/5.)])\n",
    "#0.7219280948873623\n",
    "\n",
    "probYoung=float(freqYoung)/len(inData)\n",
    "entPYoung=probYoung*entYoung\n",
    "print \"Young 엔트로피\",entPYoung,\"=\",probYoung,\"곱하기\",entYoung\n",
    "entAge=entPYoung\n",
    "\n",
    "probMiddle=float(freqMiddle)/len(inData)\n",
    "entPMiddle=probMiddle*entMiddle\n",
    "print \"Middle 엔트로피\",entPMiddle,\"=\",probMiddle,\"곱하기\",entMiddle\n",
    "entAge+=entPMiddle\n",
    "\n",
    "probOld=float(freqOld)/len(inData)\n",
    "entPOld=probOld*entOld\n",
    "print \"Old 엔트로피\",entPOld,\"=\",probOld,\"곱하기\",entOld\n",
    "entAge+=entPOld\n",
    "\n",
    "print \"나이 엔트로피:\",entAge,\"=\",entPYoung,\"+\",entPMiddle,\"+\",entPOld\n",
    "baseEntropy=sum([-p*math.log(p,2) for p in (9/15.,6/15.)])\n",
    "print \"전체 클래스 엔트로피:\",baseEntropy\n",
    "print \"나이 information gain:\",baseEntropy-entAge,\"=\",baseEntropy,\"-\",entAge\n",
    "#IG(D,age) = 0.971 - 0.888 = 0.083\n",
    "#IG(D,own_house) = 0.971 - 0.551 = 0.420\n",
    "#IG(D,has_job) = 0.971 - 0.647 = 0.324\n",
    "#IG(D,credit_rating) = 0.971 - 0.608 = 0.363\n",
    "#IG가 제일 큰 own_house가 분기하기 가장 좋은 속성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 함수로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t확률 [ 0.4  0.6] 전체빈도 15\n",
      "\t엔트로피는 0.970950594455\n",
      ">>나이 p=[ 0.33333333  0.33333333  0.33333333] allFreq=15\n",
      "\n",
      "\t확률 [ 0.4  0.6] 전체빈도 5\n",
      "\t엔트로피는 0.970950594455\n",
      "keyToSearch=middle 속성엔트로피=[ 0.3236502  0.         0.         0.       ]\n",
      "\n",
      "\t확률 [ 0.2  0.8] 전체빈도 5\n",
      "\t엔트로피는 0.721928094887\n",
      "keyToSearch=old 속성엔트로피=[ 0.5642929  0.         0.         0.       ]\n",
      "\n",
      "\t확률 [ 0.6  0.4] 전체빈도 5\n",
      "\t엔트로피는 0.970950594455\n",
      "keyToSearch=young 속성엔트로피=[ 0.88794309  0.          0.          0.        ]\n",
      "<<나이 InfoGain=[ 0.0830075  0.         0.         0.       ]\n",
      "\n",
      "\t확률 [ 0.4  0.6] 전체빈도 15\n",
      "\t엔트로피는 0.970950594455\n",
      ">>0th col p=[ 0.33333333  0.33333333  0.33333333] allFreq=15\n",
      "\n",
      "\t확률 [ 0.4  0.6] 전체빈도 5\n",
      "\t엔트로피는 0.970950594455\n",
      "keyToSearch=middle 속성엔트로피=[ 0.3236502  0.         0.         0.       ]\n",
      "\n",
      "\t확률 [ 0.2  0.8] 전체빈도 5\n",
      "\t엔트로피는 0.721928094887\n",
      "keyToSearch=old 속성엔트로피=[ 0.5642929  0.         0.         0.       ]\n",
      "\n",
      "\t확률 [ 0.6  0.4] 전체빈도 5\n",
      "\t엔트로피는 0.970950594455\n",
      "keyToSearch=young 속성엔트로피=[ 0.88794309  0.          0.          0.        ]\n",
      "<<0th InfoGain=[ 0.0830075  0.         0.         0.       ]\n",
      ">>1th col p=[ 0.66666667  0.33333333] allFreq=15\n",
      "\n",
      "\t확률 [ 0.6  0.4] 전체빈도 10\n",
      "\t엔트로피는 0.970950594455\n",
      "keyToSearch=false 속성엔트로피=[ 0.0830075  0.6473004  0.         0.       ]\n",
      "\n",
      "\t확률 [ 1.] 전체빈도 5\n",
      "\t엔트로피는 0.0\n",
      "keyToSearch=true 속성엔트로피=[ 0.0830075  0.6473004  0.         0.       ]\n",
      "<<1th InfoGain=[ 0.0830075  0.3236502  0.         0.       ]\n",
      ">>2th col p=[ 0.6  0.4] allFreq=15\n",
      "\n",
      "\t확률 [ 0.66666667  0.33333333] 전체빈도 9\n",
      "\t엔트로피는 0.918295834054\n",
      "keyToSearch=false 속성엔트로피=[ 0.0830075  0.3236502  0.5509775  0.       ]\n",
      "\n",
      "\t확률 [ 1.] 전체빈도 6\n",
      "\t엔트로피는 0.0\n",
      "keyToSearch=true 속성엔트로피=[ 0.0830075  0.3236502  0.5509775  0.       ]\n",
      "<<2th InfoGain=[ 0.0830075   0.3236502   0.41997309  0.        ]\n",
      ">>3th col p=[ 0.26666667  0.33333333  0.4       ] allFreq=15\n",
      "\n",
      "\t확률 [ 1.] 전체빈도 4\n",
      "\t엔트로피는 0.0\n",
      "keyToSearch=excellent 속성엔트로피=[ 0.0830075   0.3236502   0.41997309  0.        ]\n",
      "\n",
      "\t확률 [ 0.8  0.2] 전체빈도 5\n",
      "\t엔트로피는 0.721928094887\n",
      "keyToSearch=fair 속성엔트로피=[ 0.0830075   0.3236502   0.41997309  0.2406427 ]\n",
      "\n",
      "\t확률 [ 0.33333333  0.66666667] 전체빈도 6\n",
      "\t엔트로피는 0.918295834054\n",
      "keyToSearch=good 속성엔트로피=[ 0.0830075   0.3236502   0.41997309  0.60796103]\n",
      "<<3th InfoGain=[ 0.0830075   0.3236502   0.41997309  0.36298956]\n"
     ]
    }
   ],
   "source": [
    "inData=S\n",
    "baseEntropy=getEntropy1(inData[:,-1]) #마지막 열 (클래스)\n",
    "max_feature=inData.shape[1]-1 #마지막 열(클래스) 제외\n",
    "InfoGain=np.zeros([max_feature]) #속성별\n",
    "#속성별 key에 대해 분류 확률(즉, 나이young인 경우 대출yes인 확률)\n",
    "n_feature=0 # 첫째 속성 나이\n",
    "keyCounts=findKeyCounts(inData[:,n_feature]) #나이 열의 키값과 빈도\n",
    "max_key=keyCounts.shape[1] #나이 old,middle,young key는 3개\n",
    "allFreq=keyCounts[1,:].astype('int').sum() #전체 15개\n",
    "prob=keyCounts[1,:].astype('float')/allFreq # [5/15,5/15,5/15]\n",
    "print \">>나이 p={0} allFreq={1}\".format(prob,allFreq)\n",
    "#key(old,middle,young)를 찾아 데이터 잘라서 확률,엔트로피\n",
    "for n_key in range(max_key):\n",
    "    keyToSearch=keyCounts[0][n_key]\n",
    "    subData=inData[inData[:,n_feature]==keyToSearch]\n",
    "    # 자른 데이터의 클래스 키,빈도 구함\n",
    "    InfoGain[n_feature]+=prob[n_key]*getEntropy1(subData[:,-1])\n",
    "    print \"keyToSearch={0} 속성엔트로피={1}\".format(keyToSearch,InfoGain)\n",
    "InfoGain[n_feature]=baseEntropy-InfoGain[n_feature]\n",
    "print \"<<나이 InfoGain={0}\".format(InfoGain)\n",
    "\n",
    "#위를 함수로 만들면\n",
    "def getInfoGain(inData):\n",
    "    baseEntropy=getEntropy1(inData[:,-1]) #마지막 열 (클래스)\n",
    "    max_feature=inData.shape[1]-1 #마지막 열(클래스) 제외\n",
    "    InfoGain=np.zeros([max_feature]) #속성별\n",
    "    #속성별 key에 대해 분류 확률(즉, 나이young인 경우 대출yes인 확률)\n",
    "    for n_feature in range(max_feature):\n",
    "        keyCounts=findKeyCounts(inData[:,n_feature]) #속성 열의 키값과 빈도\n",
    "        max_key=keyCounts.shape[1] #unique key 값이 몇 개인지\n",
    "        allFreq=keyCounts[1,:].astype('int').sum()\n",
    "        prob=keyCounts[1,:].astype('float')/allFreq\n",
    "        print \">>{0}th col p={1} allFreq={2}\".format(n_feature,prob,allFreq)\n",
    "        #key를 찾아 데이터 잘라서 확률,엔트로피\n",
    "        for n_key in range(max_key):\n",
    "            keyToSearch=keyCounts[0][n_key]\n",
    "            subData=inData[inData[:,n_feature]==keyToSearch]\n",
    "            # 자른 데이터의 클래스 키,빈도 구함\n",
    "            InfoGain[n_feature]+=prob[n_key]*getEntropy1(subData[:,-1])\n",
    "            print \"keyToSearch={0} 속성엔트로피={1}\".format(keyToSearch,InfoGain)\n",
    "        InfoGain[n_feature]=baseEntropy-InfoGain[n_feature]\n",
    "        print \"<<{0}th InfoGain={1}\".format(n_feature,InfoGain)\n",
    "\n",
    "getInfoGain(inData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decision Tree 구조 및 5. 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## scikit ML 패키지를 사용하여:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "clf.predict([[2., 2.]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 교재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy= 0.970950594455\n",
      "{'yes': 1}\n",
      "0\n",
      "['flippers']\n",
      "False\n",
      "testVec=[1, 0]을 분류하면 결과 class label=no\n",
      "{'No': {'middle': {'Yes': {'false': {'No': {'good': 'good', 'fair': 'fair'}}, 'true': {'No': {'true': 'good', 'false': 'excellent'}}}}, 'old': {'No': {'false': {'Yes': {'true': 'good', 'false': 'fair'}}, 'true': {'No': {'good': 'good', 'excellent': 'excellent'}}}}, 'young': {'Yes': {'false': {'No': {'false': 'fair', 'true': 'good'}}, 'true': 'fair'}}}}\n"
     ]
    }
   ],
   "source": [
    "# learn textbook 엔트로피 계산.\n",
    "import os\n",
    "os.chdir('/Users/media/Code/git/else/machinelearninginaction/Ch03')\n",
    "import trees\n",
    "myDat,labels=trees.createDataSet()\n",
    "#myDat [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n",
    "#labels ['no surfacing', 'flippers']\n",
    "print \"entropy=\",trees.calcShannonEnt(myDat) #0.9709505944546686\n",
    "\n",
    "#inside calcShannonEnt: 학습데이터 1개 행의 클래스의 빈도 계산\n",
    "numEntries=len(myDat)\n",
    "labelCounts={}\n",
    "currentLabel=myDat[0][-1] #[1, 1, 'yes']의 마지막 'yes'\n",
    "if currentLabel not in labelCounts.keys():\n",
    "    labelCounts[currentLabel]=0\n",
    "labelCounts[currentLabel]+=1\n",
    "print labelCounts #{'yes': 1}\n",
    "\n",
    "print trees.chooseBestFeatureToSplit(myDat)\n",
    "\n",
    "\n",
    "# create tree as a dictionary and search\n",
    "#createTree p.48\n",
    "myTree=trees.createTree(myDat,labels)\n",
    "#{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n",
    "print labels #createTree하면 labels lost ['flippers'], so reload!!!\n",
    "\n",
    "#insdie classify p.56\n",
    "myDat,labels=trees.createDataSet() #now labels ['no surfacing', 'flippers']\n",
    "firstStr=myTree.keys()[0] #'no surfacing'\n",
    "secondDict=myTree[firstStr] #{0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}\n",
    "featIndex=labels.index(firstStr) #0\n",
    "secondDict.keys() #[0, 1]\n",
    "secondDict #{0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}\n",
    "testVec=[1,0]\n",
    "key=secondDict.keys()[0] #0\n",
    "print testVec[featIndex]==key #False\n",
    "print \"testVec={0}을 분류하면 결과 class label={1}\".format(testVec,secondDict[0])\n",
    "\n",
    "# sample data로\n",
    "S=np.array([['young', 'false', 'false', 'fair', 'No'],\n",
    "       ['young', 'false', 'false', 'good', 'No'],\n",
    "       ['young', 'true', 'false', 'good', 'Yes'],\n",
    "       ['young', 'true', 'true', 'fair', 'Yes'],\n",
    "       ['young', 'false', 'false', 'fair', 'No'],\n",
    "       ['middle', 'false', 'false', 'fair', 'No'],\n",
    "       ['middle', 'false', 'false', 'good', 'No'],\n",
    "       ['middle', 'true', 'true', 'good', 'Yes'],\n",
    "       ['middle', 'false', 'true', 'excellent', 'Yes'],\n",
    "       ['middle', 'false', 'true', 'excellent', 'Yes'],\n",
    "       ['old', 'false', 'true', 'excellent', 'Yes'],\n",
    "       ['old', 'false', 'true', 'good', 'Yes'],\n",
    "       ['old', 'true', 'false', 'good', 'Yes'],\n",
    "       ['old', 'true', 'false', 'excellent', 'Yes'],\n",
    "       ['old', 'false', 'false', 'fair', 'No']], \n",
    "      dtype='|S9')\n",
    "dat=S[:,:-1].tolist()\n",
    "lab=S[:,-1].tolist()\n",
    "print trees.createTree(dat,lab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
